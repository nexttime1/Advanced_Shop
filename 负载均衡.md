### 细节

服务发现和负载均衡的细节 

```
resolver 负责 告诉 gRPC：现在有哪些服务地址
 balancer 负责 为这些地址建立连接 & 管理连接状态
 picker 负责 每一次 RPC 用哪一条连接
```



![image-20260122222259965](C:\Users\20839\AppData\Roaming\Typora\typora-user-images\image-20260122222259965.png)

## 负载均衡

### 自己实现接口的源码解读

#### 全局解释器

可以使用 `SetGlobalSelector` 方法去声明使用什么算法 就不会写死 `random` 或者 `p2c` 等等

```GO
var globalSelector Builder  // 全局变量，存储当前使用的 Builder  这个是 pickerBuilder 

func GlobalSelector() Builder {
    return globalSelector
}

func SetGlobalSelector(builder Builder) {
    globalSelector = builder
}
```

#### 必要的一些接口

想成为全局解释器 就必须实现 `Build` 方法 本质上就是可以初始化一个选择器

`select` 也是一个接口  这样写的好处就是自由度高，扩展性强，缺点就是理解有点麻烦，接口多了就可能会乱

```go
type Builder interface {
    Build() Selector  // 创建一个 Selector
}

type Selector interface {
	Rebalancer          // Go 的 “组合优于继承” 思想  匿名嵌套接口   代码复用 扩展性强
	// Select nodes
	// if err == nil, selected and done must not be empty.
	Select(ctx context.Context) (selected Node, done DoneFunc, err error)
}

type Rebalancer interface {
	// Apply is apply all nodes when any changes happen
	Apply(nodes []Node)
}
```

#### 向 `GlobalSelector`  这里去放 `builder` 

他俩对于 实现的pick 不一样 Node 不一样   返回的都是这个 `DefaultBuilder`  

这样 比如在实现一种策略 `wrr` 同样返回 `DefaultBuilder` 扩展性高 代码耦合度低

```GO
// random 的话
func NewBuilder() selector.Builder {
	return &selector.DefaultBuilder{
		Balancer: &Builder{},  // random自己定义的 只要实现pick接口就行
		Node:     &direct.Builder{},
	}
}
// p2c 的话
func NewBuilder() selector.Builder {
	return &selector.DefaultBuilder{
		Balancer: &Builder{}, // p2c 自己定义的 只要实现pick接口就行
		Node:     &ewma.Builder{},
	}
}
```

#### `DefaultBuilder`  函数

后面会用到  **最能绕晕的函数**

```GO
func (d *Default) Apply(nodes []Node) {
	weightedNodes := make([]WeightedNode, 0, len(nodes))
	for _, n := range nodes {
		weightedNodes = append(weightedNodes, d.NodeBuilder.Build(n))
	}
	// TODO: Do not delete unchanged nodes
	d.nodes.Store(weightedNodes)
}

// DefaultBuilder is de
type DefaultBuilder struct {
	Node     WeightedNodeBuilder
	Balancer BalancerBuilder
}

// 重要 ！！！ 刚开始 InitBuilder 也就是下一个的函数  
func (db *DefaultBuilder) Build() Selector {
	return &Default{
		NodeBuilder: db.Node,
		Balancer:    db.Balancer.Build(),
	}
}
```

#### 初始化`Builder` 

当然 执行这个的时候需要  调用 `SetGlobalSelector`  方法 把定义的实例放进去，后面会再说

```GO
func InitBuilder() {
	b := base.NewBalancerBuilder(
		balancerName,  // selector  也就是Dial传的
		&balancerBuilder{  // 我自己实现的 PickerBuilder 这个接口 就可以传
			builder: selector.GlobalSelector(),  // 
		},
		base.Config{HealthCheck: true},
	)
	balancer.Register(b)   // 同样注册到 grpc 的 map 中
}

```

#### 用`grpc` 的 `base`

它的基础结构体已经够用，`grpc` 也有其他的，扩展的，字段丰富的，但好像用不到就直接使用  `base`

```GO
func NewBalancerBuilder(name string, pb PickerBuilder, config Config) balancer.Builder {
	return &baseBuilder{
		name:          name,
		pickerBuilder: pb,
		config:        config,
	}
}
```

在拨号的时候 用了这个方法 `WithDefaultServiceConfig`  创建`Client` 的时候就会解析

```GO
	grpcOpts := []grpc.DialOption{
        // 重点！！！                           options.balancerName： selector
		grpc.WithDefaultServiceConfig(`{"loadBalancingPolicy": "` + options.balancerName + `"}`),
		grpc.WithChainUnaryInterceptor(ints...),
		grpc.WithChainStreamInterceptor(streamInts...),
	}
```

### `NewClient` 函数 （相关部分）

```go
func NewClient(target string, opts ...DialOption) (conn *ClientConn, err error) {
    //  初始化负载均衡选择器包装器
	cc.pickerWrapper = newPickerWrapper()
    
    // 解析用户传入的 JSON 格式服务配置（比如负载均衡策略、重试次数）；
    // defaultServiceConfigRawJSON：用户传入的 JSON 字符串（自己实现的 {"loadBalancingPolicy":"selector"}）；
	// parseServiceConfig：将 JSON 转换成 gRPC 内部的 ServiceConfig 结构体（方便后续使用）。
    // 最终返回 return &serviceconfig.ParseResult{Config: &sc}  sc里面有自己定义的 balancerBuiler
	if cc.dopts.defaultServiceConfigRawJSON != nil {
        // maxCallAttempts 为 grpc 配置的最大调用重试次数
		scpr := parseServiceConfig(*cc.dopts.defaultServiceConfigRawJSON, cc.dopts.maxCallAttempts)
		if scpr.Err != nil {
			return nil, fmt.Errorf("%s: %v", invalidDefaultServiceConfigErrPrefix, scpr.Err)
		}
        // 这个是重点  sc 给了 cc.dopts.defaultServiceConfig,   sc.lbConfig 是我们的 balancerBuiler
		cc.dopts.defaultServiceConfig, _ = scpr.Config.(*ServiceConfig)
	}
    
    // 创建连接状态管理器，负责维护 ClientConn 的整体状态（Idle/Connecting/Ready 等）；
    // 维护 ClientConn 的整体状态（Idle/Ready/TransientFailure 等），只有Ready状态的节点才会被纳入负载均衡选择范围；
	cc.csMgr = newConnectivityStateManager(cc.ctx, cc.channelz)
    
    cc.pickerWrapper = newPickerWrapper()
    
    // 管理自研Balancer的创建 / 销毁，接收解析器推送的节点列表，监听节点连接状态（如某个节点断连则标记不可用）；
    /*
    cc.resolverWrapper = newCCResolverWrapper(cc)
	cc.balancerWrapper = newCCBalancerWrapper(cc)
	cc.firstResolveEvent = grpcsync.NewEvent()
    */
	cc.initIdleStateLocked()
}

```

### `parseServiceConfig` 核心函数 

```GO
func parseServiceConfig(js string, maxAttempts int) *serviceconfig.ParseResult {
	if len(js) == 0 {  // 用于以防万一         我们是  {"loadBalancingPolicy":"selector"}
		return &serviceconfig.ParseResult{Err: fmt.Errorf("no JSON service config provided")}
	}
	var rsc jsonSC  // 这是个结构体 接收 我们的{"loadBalancingPolicy":"selector"}
	err := json.Unmarshal([]byte(js), &rsc)  
	if err != nil {
		logger.Warningf("grpc: unmarshalling service config %s: %v", js, err)
		return &serviceconfig.ParseResult{Err: err}
	}
    // rsc.LoadBalancingPolicy 设为指向字符串 "selector" 的指针，其他字段均为 nil。
	sc := ServiceConfig{
		Methods:           make(map[string]MethodConfig), // 空
		retryThrottling:   rsc.RetryThrottling, // 空
		healthCheckConfig: rsc.HealthCheckConfig,  // 空
  		rawJSONString:     js,   // 原生的 留着当日志
	}
    
    c := rsc.LoadBalancingConfig  // 这个我没配置 我们只有 selector 给到 LoadBalancingPolicy 字段
    if c == nil {
        // 设置一个grpc默认的 后面自己改
        name := pickfirst.Name  
        if rsc.LoadBalancingPolicy != nil {
            name = *rsc.LoadBalancingPolicy  // 覆盖为自己配置的"selector"
        }
        // 校验策略是否已注册（核心！）
        if balancer.Get(name) == nil { // 这个我们注册了 init函数
            name = pickfirst.Name  // 未注册则回退到默认策略
        }
        // 步骤4.3：构建gRPC标准的LB配置格式
        cfg := []map[string]any{{name: struct{}{}}}  // 格式：[{"selector": {}}]
        strCfg, err := json.Marshal(cfg)
        if err != nil {
            return &serviceconfig.ParseResult{Err: fmt.Errorf("unexpected error marshaling simple LB config: %w", err)}
        }
        r := json.RawMessage(strCfg) // 由于刚开始c为空 我们要构建c的类型
        c = &r  // 将构建好的LB配置赋值给c 
    }
    // 解析LB配置为gRPC可识别的格式    重点方法 ParseConfig 
    cfg, err := gracefulswitch.ParseConfig(*c)
    if err != nil {
        return &serviceconfig.ParseResult{Err: err}
    }
    sc.lbConfig = cfg  // 将解析后的LB配置存入ServiceConfig  cfg里面有我自己的builder 负载均衡的

	if rsc.MethodConfig == nil {
		return &serviceconfig.ParseResult{Config: &sc}
	}
	// 处理MethodConfig的详细逻辑，包括重试、超时、消息大小限制等  我们不走
    /*
    针对特定 RPC 方法的个性化配置（比如 {"methodConfig": [{"name": [{"service": "xxx", "method": "yyy"}], 			"timeout": "1s"}]}）。
    */
	paths := map[string]struct{}{}
	for _, m := range *rsc.MethodConfig {
		if m.Name == nil {
			continue
		}

		mc := MethodConfig{
			WaitForReady: m.WaitForReady,
			Timeout:      (*time.Duration)(m.Timeout),
		}
		if mc.RetryPolicy, err = convertRetryPolicy(m.RetryPolicy, maxAttempts); err != nil {
			logger.Warningf("grpc: unmarshalling service config %s: %v", js, err)
			return &serviceconfig.ParseResult{Err: err}
		}
		if m.MaxRequestMessageBytes != nil {
			if *m.MaxRequestMessageBytes > int64(maxInt) {
				mc.MaxReqSize = newInt(maxInt)
			} else {
				mc.MaxReqSize = newInt(int(*m.MaxRequestMessageBytes))
			}
		}
		if m.MaxResponseMessageBytes != nil {
			if *m.MaxResponseMessageBytes > int64(maxInt) {
				mc.MaxRespSize = newInt(maxInt)
			} else {
				mc.MaxRespSize = newInt(int(*m.MaxResponseMessageBytes))
			}
		}
		for i, n := range *m.Name {
			path, err := n.generatePath()
			if err != nil {
				logger.Warningf("grpc: error unmarshalling service config %s due to methodConfig[%d]: %v", js, i, err)
				return &serviceconfig.ParseResult{Err: err}
			}

			if _, ok := paths[path]; ok {
				err = errDuplicatedName
				logger.Warningf("grpc: error unmarshalling service config %s due to methodConfig[%d]: %v", js, i, err)
				return &serviceconfig.ParseResult{Err: err}
			}
			paths[path] = struct{}{}
			sc.Methods[path] = mc
		}
	}
	// 重试限流（RetryThrottling）配置校验
	if sc.retryThrottling != nil {
		if mt := sc.retryThrottling.MaxTokens; mt <= 0 || mt > 1000 {
			return &serviceconfig.ParseResult{Err: fmt.Errorf("invalid retry throttling config: maxTokens (%v) out of range (0, 1000]", mt)}
		}
		if tr := sc.retryThrottling.TokenRatio; tr <= 0 {
			return &serviceconfig.ParseResult{Err: fmt.Errorf("invalid retry throttling config: tokenRatio (%v) may not be negative", tr)}
		}
	}
	return &serviceconfig.ParseResult{Config: &sc}
}

```

### `gracefulswitch.ParseConfig` 重点函数

```go
// 入参cfg：对于我们来说是 json.RawMessage 类型，值为 `[{"selector":{}}]` 的字节流
func ParseConfig(cfg json.RawMessage) (serviceconfig.LoadBalancingConfig, error) {
    // 反序列化LB配置为 []map[string]json.RawMessage
	var lbCfg []map[string]json.RawMessage
	if err := json.Unmarshal(cfg, &lbCfg); err != nil {
		return nil, err
	}

    // 遍历LB配置数组（例子来说  只有1个元素）
	for i, e := range lbCfg {
        // 校验每个配置项只能有1个策略名（gRPC强制要求）
		if len(e) != 1 {
			return nil, fmt.Errorf("expected a JSON struct with one entry; received entry %v at index %d", e, i)
		}

        // 提取策略名和对应的配置（map只有1个键值对，循环一次就拿到）
		var name string
		var jsonCfg json.RawMessage
		for name, jsonCfg = range e {
		}

        // 重点 从注册中心获取你注册的selector构建器
		builder := balancer.Get(name)
		if builder == nil {
			// 未注册则跳过
			continue
		}

        // 检查构建器是否需要解析专属配置 
		parser, ok := builder.(balancer.ConfigParser)
		if !ok {
			// 我没实现该接口，直接返回包含构建器的lbConfig  我们直接返回
			return &lbConfig{childBuilder: builder}, nil
		}

        // 解析策略专属配置   没实现这个  也不需要额外配置
		cfg, err := parser.ParseConfig(jsonCfg)
		if err != nil {
			return nil, fmt.Errorf("error parsing config for policy %q: %v", name, err)
		}
		return &lbConfig{childBuilder: builder, childConfig: cfg}, nil
	}

    //  遍历完没找到支持的策略，返回错误
	return nil, fmt.Errorf("no supported policies found in config: %v", string(cfg))
}
```

### `newPickerWrapper` 函数

```go
func newPickerWrapper() *pickerWrapper {
    pw := &pickerWrapper{}  //初始化 picker 设置为 nil 
    pw.pickerGen.Store(&pickerGeneration{
       blockingCh: make(chan struct{}),   // 呼唤器：唤醒所有等它的 goroutine
    })
    return pw
}

type pickerGeneration struct {
    picker balancer.Picker
    // blockingCh is closed when the picker has been invalidated because there
    // is a new one available.
    blockingCh chan struct{}
}

// pickerWrapper is a wrapper of balancer.Picker. It blocks on certain pick
// actions and unblock when there's a picker update.
type pickerWrapper struct {
    // If pickerGen holds a nil pointer, the pickerWrapper is closed.
    pickerGen atomic.Pointer[pickerGeneration]
}
```

### `newCCBalancerWrapper` 函数

```go
func newCCBalancerWrapper(cc *ClientConn) *ccBalancerWrapper {
	ctx, cancel := context.WithCancel(cc.ctx)
	ccb := &ccBalancerWrapper{
		cc: cc,   // 主conn
		opts: balancer.BuildOptions{  // 默认以及我们自己的配置
			DialCreds:       cc.dopts.copts.TransportCredentials,
			CredsBundle:     cc.dopts.copts.CredsBundle,
			Dialer:          cc.dopts.copts.Dialer, 
			Authority:       cc.authority,
			CustomUserAgent: cc.dopts.copts.UserAgent,
			ChannelzParent:  cc.channelz,
			Target:          cc.parsedTarget,
		},
		serializer:       grpcsync.NewCallbackSerializer(ctx),
		serializerCancel: cancel,
	}
	// 后续会用到
	ccb.balancer = gracefulswitch.NewBalancer(ccb, ccb.opts)
	return ccb
}
```

### `NewBalancer`  函数

```go
func NewBalancer(cc balancer.ClientConn, opts balancer.BuildOptions) *Balancer {
    // gRPC 用来支持运行时平滑切换负载均衡策略的代理层。
	return &Balancer{
		cc:    cc, // 直接把 ccBalancerWrapper 里面有父挂件 ClientConn 包装进来
		bOpts: opts,
	}
}
```

### 

### `DialContext` 函数  （相关部分）

```GO
func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {
  
	/*
    2. 创建负载均衡器
    解析器拿到 IP 列表后，enforcer 会根据服务配置（比如你设置的 round_robin 轮询策略）；
    调用负载均衡器的 Builder 创建负载均衡器实例；
    负载均衡器开始工作：管理后端 IP 列表 → 选择下一个要连接的 IP → 为这个 IP 创建底层连接。
    3. 初始化连接池
    负载均衡器选好 IP 后，enforcer 会创建 addrConn（底层 TCP 连接）；
    把这些 addrConn 加入 cc.conns（NewClient 里初始化的 make(map[*addrConn]struct{})）；
    形成连接池：后续 RPC 调用从连接池里选可用的连接发请求。
    */
	if err := cc.idlenessMgr.ExitIdleMode(); err != nil {
		return nil, err
	}
    
   
}
```



### 深入了解  `cc.idlenessMgr.ExitIdleMode()`  这个函数

```GO
func (m *Manager) ExitIdleMode() error { //这个函数是跳出 空闲状态 因为来订阅者了
	// Holds idleMu which ensures mutual exclusion with tryEnterIdleMode.
	m.idleMu.Lock()
	defer m.idleMu.Unlock()
	// 这里面 先判断如果连接已经关了，或者本来就不是真的空闲（比如已经在连接中），就啥也不用干，直接返回，
	if m.isClosed() || !m.actuallyIdle {
		return nil
	}
    // 如果是空闲 那就调用m.enforcer.ExitIdleMode() 而这个是接口 
	if err := m.enforcer.ExitIdleMode(); err != nil {
		return fmt.Errorf("failed to exit idle mode: %w", err)
	}

	// Undo the idle entry process. This also respects any new RPC attempts.
	atomic.AddInt32(&m.activeCallsCount, math.MaxInt32)
	m.actuallyIdle = false

	// Start a new timer to fire after the configured idle timeout.
	m.resetIdleTimerLocked(m.timeout)
	return nil
}

```

`enforcer`  是下面结构体赋值的 最终调用的是  `cc.exitIdleMode` 函数

```go
func (i *idler) ExitIdleMode() error {
	return (*ClientConn)(i).exitIdleMode()
}
```

 `cc.exitIdleMode` 函数

```GO
func (cc *ClientConn) exitIdleMode() (err error) {
	cc.mu.Lock()
	if cc.conns == nil {
		cc.mu.Unlock()
		return errConnClosing
	}
	cc.mu.Unlock()

	// This needs to be called without cc.mu because this builds a new resolver
	// which might update state or report error inline, which would then need to
	// acquire cc.mu.
	if err := cc.resolverWrapper.start(); err != nil {
		return err
	}

	cc.addTraceEvent("exiting idle mode")
	return nil
}

```

### `start` 函数

```go
func (ccr *ccResolverWrapper) start() error {
	errCh := make(chan error)  // 因为下面是异步 所以要阻塞一下
	ccr.serializer.TrySchedule(func(ctx context.Context) {
		if ctx.Err() != nil {  // clientconn 给挂了 那没必要了
			return
		}
		opts := resolver.BuildOptions{  // 封装一下
			DisableServiceConfig: ccr.cc.dopts.disableServiceConfig,
			DialCreds:            ccr.cc.dopts.copts.TransportCredentials,
			CredsBundle:          ccr.cc.dopts.copts.CredsBundle,
			Dialer:               ccr.cc.dopts.copts.Dialer,
			Authority:            ccr.cc.authority,
			MetricsRecorder:      ccr.cc.metricsRecorderList,
		}
		var err error
		//第一个分支 注册的 resolver 直接生效    gRPC 不插手  不想要任何 gRPC 默认行为  我们用这个
        // 使用的话 grpc.WithNoProxy() grpc.WithContextDialer(...)
		if ccr.cc.dopts.copts.Dialer != nil || !ccr.cc.dopts.useProxy {
			ccr.resolver, err = ccr.cc.resolverBuilder.Build(ccr.cc.parsedTarget, ccr, opts)
		} else {
            // 第二个分支 grpc 会帮你  Proxy 判断 本地 DNS 优化 scheme fallback  我们虽然走这个 但是其实走上面的
			ccr.resolver, err = delegatingresolver.New(ccr.cc.parsedTarget, ccr, opts, ccr.cc.resolverBuilder, ccr.cc.dopts.enableLocalDNSResolution)
		}
		errCh <- err
	})
	return <-errCh
}
```

```
执行我自己的build方法 然后经过一系列操作那是服务发现的逻辑这边简单化 最终拿到 实例我自己定义的实例 通知给grpc要转成他自己的，也就是执行下面这个函数
```

### `update`   函数 调用 `UpdateState` 

```GO
func (r *discoveryResolver) update(ins []*registry.ServiceInstance) {
	addrs := make([]resolver.Address, 0)
	endpoints := make(map[string]struct{})
	for _, in := range ins {
		endpoint, err := ParseEndpoint(in.Endpoints, "grpc", !r.insecure)
		if err != nil {
			log.Errorf("[resolver] Failed to parse discovery endpoint: %v", err)
			continue
		}
		if endpoint == "" {
			continue
		}
		// filter redundant endpoints
		if _, ok := endpoints[endpoint]; ok {
			continue
		}
		endpoints[endpoint] = struct{}{}
		addr := resolver.Address{
			ServerName: in.Name,
			Attributes: parseAttributes(in.Metadata),
			Addr:       endpoint,
		}
		addr.Attributes = addr.Attributes.WithValue("rawServiceInstance", in)
		addrs = append(addrs, addr)
	}
	if len(addrs) == 0 {
		log.Warnf("[resolver] Zero endpoint found,refused to write, instances: %v", ins)
		return
	}
	err := r.cc.UpdateState(resolver.State{Addresses: addrs}) // gRPC 客户端更新地址列表
	if err != nil {
		log.Errorf("[resolver] failed to update state: %s", err)
	}
	b, _ := json.Marshal(ins)
	log.Infof("[resolver] update instances: %s", b)
}
```

### `UpdateState` 函数

```GO
// UpdateState is called by resolver implementations to report new state to gRPC
// which includes addresses and service config.
func (ccr *ccResolverWrapper) UpdateState(s resolver.State) error {
	ccr.cc.mu.Lock()
	ccr.mu.Lock()
	if ccr.closed {
		ccr.mu.Unlock()
		ccr.cc.mu.Unlock()
		return nil
	}
    // 兼容处理
	if s.Endpoints == nil {
		s.Endpoints = addressesToEndpoints(s.Addresses)
	}
	ccr.addChannelzTraceEvent(s)
	ccr.curState = s
	ccr.mu.Unlock()
    // 把解析器状态转发给 ClientConn 的核心处理函数  解锁在函数里面   ccr.cc.mu.Lock()
    // 这里的s 就是 结构体 里面有个 addresses 字段 就是全部的我自己 registry.ServiceInstance 转换的
	return ccr.cc.updateResolverStateAndUnlock(s, nil)
}
```

### `updateResolverStateAndUnlock` 函数

```GO
func (cc *ClientConn) updateResolverStateAndUnlock(s resolver.State, err error) error {
	// 触发 firstResolveEvent：通知 DialContext 解析器已完成第一次解析（阻塞等待的逻辑会感知）
	defer cc.firstResolveEvent.Fire()

	// 防御：如果 ClientConn 已关闭（conns 置 nil 是关闭标志），直接返回
	if cc.conns == nil {
		cc.mu.Unlock()
		return nil
	}

	// ========== 分支1：解析器返回错误（比如注册中心（etcd/nacos）宕机 ==========
	if err != nil {
		// 应用默认服务配置
		cc.maybeApplyDefaultServiceConfig()
		// 通知负载均衡器：解析器出错，触发失败逻辑（比如标记所有节点不可用）
		cc.balancerWrapper.resolverError(err)
		// 无有效地址，返回错误
		cc.mu.Unlock()
		return balancer.ErrBadResolverState
	}

	var ret error
	// ========== 分支2：禁用服务配置（用户显式配置 WithDisableServiceConfig） ==========
	if cc.dopts.disableServiceConfig {
		channelz.Infof(logger, cc.channelz, "ignoring service config from resolver (%v) and applying the default because service config is disabled", s.ServiceConfig)
        // 不管 resolver 返回什么配置，我都不要   应用默认服务配置
		cc.maybeApplyDefaultServiceConfig()
	// ========== 分支3：resolver 只推了地址列表，没带 ServiceConfig==========
	} else if s.ServiceConfig == nil {// 我们这个就是
		cc.maybeApplyDefaultServiceConfig()
	} else {
        	/*  当传入了 就走分支4  主要是 	动态，可以热更新   动态指定负载均衡策略
	r.cc.UpdateState(resolver.State{
    Addresses: addrs,
    ServiceConfig: &serviceconfig.ParseResult{
        Config: &ServiceConfig{
            lbConfig: ..., // 动态指定负载均衡策略
            Methods: ...,  // 每个方法的超时、重试配置
        },
    },
})	
	*/
		// 校验服务配置是否合法（转成 gRPC 内部的 ServiceConfig 结构体）
		if sc, ok := s.ServiceConfig.Config.(*ServiceConfig); s.ServiceConfig.Err == nil && ok {
			// 获取服务配置选择器（用于选择负载均衡、重试策略）
			configSelector := iresolver.GetConfigSelector(s)
			if configSelector != nil {
				if len(s.ServiceConfig.Config.(*ServiceConfig).Methods) != 0 {
					channelz.Infof(logger, cc.channelz, "method configs in service config will be ignored due to presence of config selector")
				}
			} else {
				// 使用默认配置选择器（封装负载均衡策略）
				configSelector = &defaultConfigSelector{sc}
			}
			// ########### 关键步骤：应用服务配置 + 初始化负载均衡器 ###########
            // 调用你注册的 balancer.Builder（比如 p2c.NewBuilder()），创建 p2c.Balancer 实例；
			cc.applyServiceConfigAndBalancer(sc, configSelector)
		} else {
			// 服务配置解析失败，返回错误
			ret = balancer.ErrBadResolverState
			if cc.sc == nil {
				// 如果从未收到过有效服务配置，应用“失败负载均衡策略”（所有调用直接失败）
				cc.applyFailingLBLocked(s.ServiceConfig)
				cc.mu.Unlock()
				return ret
			}
		}
	}

	// 核心 我的负载均衡Builder就是sc.lbConfig
	balCfg := cc.sc.lbConfig
	// 拿到 NewClient 里初始化的 balancerWrapper（负载均衡器包装器） Newccb
	bw := cc.balancerWrapper  
	// 前面的 cc.mu.Lock 在这里释放
	cc.mu.Unlock()

	// 通知 balancerWrapper 更新客户端连接状态（地址列表 + 负载均衡策略）
	//    这是「解析器 → 负载均衡器」的最终传递步骤
	uccsErr := bw.updateClientConnState(&balancer.ClientConnState{
		ResolverState: s,       //  是所有的地址列表 有serviceInstance 转成 grpc喜欢的地址列表
		BalancerConfig: balCfg, // 负载均衡器  里面有builder
	})
	if ret == nil {
		ret = uccsErr // 优先返回解析器相关错误
	}
	return ret
}

```

### `maybeApplyDefaultServiceConfig` 函数

```GO
func (cc *ClientConn) maybeApplyDefaultServiceConfig() {
	if cc.sc != nil {
        // 第二次 进入  比如注册中心节点变化 直接用已有的 cc.sc，不需要重新解析默认配置
		cc.applyServiceConfigAndBalancer(cc.sc, nil)
		return
	}
	if cc.dopts.defaultServiceConfig != nil {
		cc.applyServiceConfigAndBalancer(cc.dopts.defaultServiceConfig, &defaultConfigSelector{cc.dopts.defaultServiceConfig})
	} else {
		cc.applyServiceConfigAndBalancer(emptyServiceConfig, &defaultConfigSelector{emptyServiceConfig})
	}
}
```

### `applyServiceConfigAndBalancer` 函数

```GO
func (cc *ClientConn) applyServiceConfigAndBalancer(sc *ServiceConfig, configSelector iresolver.ConfigSelector) {
    // 防御性检查
    if sc == nil {
        return
    }
    
    // 保存服务配置到 ClientConn
    // cc.sc 是当前生效的服务配置，后续 updateResolverStateAndUnlock 末尾
    // cc.dopts.defaultServiceConfig
    // 通过 balCfg := cc.sc.lbConfig 取负载均衡Builder
    cc.sc = sc
    
    // 更新配置选择器
    // configSelector 用于根据 RPC 方法名选择对应的配置（超时、重试等）
    // 如果没有按方法粒度配置也就是只有 {"loadBalancingPolicy": "selector"}，没有 methodConfig。
    // 就用 defaultConfigSelector
    if configSelector != nil {
        cc.safeConfigSelector.UpdateConfigSelector(configSelector)
    }

    // 重试限流器配置
    // 如果配置了重试限流（防止重试风暴），就创建 throttler
    if cc.sc.retryThrottling != nil {
        newThrottler := &retryThrottler{
            tokens: cc.sc.retryThrottling.MaxTokens,
            max:    cc.sc.retryThrottling.MaxTokens,
            thresh: cc.sc.retryThrottling.MaxTokens / 2,
            ratio:  cc.sc.retryThrottling.TokenRatio,
        }
        cc.retryThrottler.Store(newThrottler)
    } else {
        cc.retryThrottler.Store((*retryThrottler)(nil))
    }
    
}
```

### `updateClientConnState`  函数

```GO
func (ccb *ccBalancerWrapper) updateClientConnState(ccs *balancer.ClientConnState) error {
    // 创建 channel 用于同步等待结果
    // 因为下面的逻辑是异步执行的，需要阻塞等待
    errCh := make(chan error)
    
    // 定义真正的更新逻辑（闭包）
    uccs := func(ctx context.Context) {
        // 函数结束时关闭 channel（通知主线程）
        defer close(errCh)
        
        // 防御检查
        // ctx.Err() != nil：ClientConn 已关闭
        // ccb.balancer == nil：balancer 还没初始化
        // ccb.balancer 在上面有字段 就两个
        if ctx.Err() != nil || ccb.balancer == nil {
            return
        }
        
        //  ★★★ 关键：从配置中取出负载均衡策略名 ★★★
        // ccs.BalancerConfig 是 lbConfig  （balanceBuilder）
        // gracefulswitch.ChildName() 从中提取策略名（如 "selector"）
        name := gracefulswitch.ChildName(ccs.BalancerConfig)
        
        // 策略名变了 打日志  服务动态负载均衡的
        // 比如从 "round_robin" 切换到 "selector"
        if ccb.curBalancerName != name {
            ccb.curBalancerName = name
            channelz.Infof(logger, ccb.cc.channelz, "Channel switches to new LB policy %q", name)
        }
        
        //  ★★★ 调用 balancer 的 UpdateClientConnState ★★★
        // 它内部会：
        //   - 根据 name 从全局注册表找 Builder
        //   - 如果策略名变了，用新 Builder 创建新 balancer
        //   - 把地址列表传给真正的 balancer
        err := ccb.balancer.UpdateClientConnState(*ccs)
        
        if logger.V(2) && err != nil {
            logger.Infof("error from balancer.UpdateClientConnState: %v", err)
        }
        
        // 把结果发回 channel
        errCh <- err
    }
    
    // 失败时的回调（只是关闭 channel）
    onFailure := func() { close(errCh) }

    // ★★★ 调度执行 ★★★
    // serializer 是一个串行化器，保证多个更新操作按顺序执行
    // ScheduleOr：尝试调度 uccs，如果 serializer 已关闭就执行 onFailure

    //  为什么用 serializer？
    // - UpdateClientConnState 可能被多次调用（resolver 多次推送）
    // - 需要保证按顺序处理，避免并发问题
    // - 如果 ClientConn 正在关闭，serializer 会拒绝新任务
    ccb.serializer.ScheduleOr(uccs, onFailure)
    
    // 阻塞等待结果
    // 虽然 uccs 是异步调度的，但这里同步等待它完成
    return <-errCh
}
```

### `balancer.UpdateClientConnState` 函数

```GO
func (gsb *Balancer) UpdateClientConnState(state balancer.ClientConnState) error {
	// The resolver data is only relevant to the most recent LB Policy.
	balToUpdate := gsb.latestBalancer()
	gsbCfg, ok := state.BalancerConfig.(*lbConfig)
	if ok {
		// 首次调用 balToUpdate 为空  或者 策略名不一样：需要从 A 策略切换到 B 策略
		if balToUpdate == nil || gsbCfg.childBuilder.Name() != balToUpdate.builder.Name() {
			var err error
           /*  逻辑线
           调用parseJsonConfig 得到 首先返回 &lbConfig{childBuilder: builder}
           赋值给 sc.lbConfig  sc是ServiceConfig{}  然后 把 sc 赋值给 cc.dopts.defaultServiceConfig
           所以我的builder 是在  cc.dopts.defaultServiceConfig.lbConfig.childBuilder
           然后传过来的 state 是:
           &balancer.ClientConnState{
			ResolverState: s,       //  是所有的地址列表 有serviceInstance 转成 grpc喜欢的地址列表
			BalancerConfig: balCfg, // 负载均衡配置器lbConfig
           */
			balToUpdate, err = gsb.switchTo(gsbCfg.childBuilder)  //bw   balancerWrapper
			if err != nil {
				return fmt.Errorf("could not switch to new child balancer: %w", err)
			}
		}
		// 通用容器 替换成 子策略专属配置
		state.BalancerConfig = gsbCfg.childConfig
	}

	if balToUpdate == nil {
		return errBalancerClosed
	}

	// Perform this call without gsb.mu to prevent deadlocks if the child calls
	// back into the channel. The latest balancer can never be closed during a
	// call from the channel, even without gsb.mu held.
    //  balToUpdate  就是 bw 刚创建的  不是cc.balancerWrapper
    // 但是balancerWrapper类型没有这个方法 由于嵌入了 bw.Balancer = newBalancer  
    //  bw.Balancer这个是匿名字段 解析器就会执行balToUpdate.Balancer.UpdateClientConnState 方法
	return balToUpdate.UpdateClientConnState(state)
}

```

### `latestBalancer` 函数

```GO
func (gsb *Balancer) latestBalancer() *balancerWrapper {
    gsb.mu.Lock()
    defer gsb.mu.Unlock()
    
    /*
    当前用 round_robin 策略，balancerCurrent 是 round_robin balancer
	配置中心下发新策略 selector，开始切换
	切换过程中 balancerPending 是新的 selector balancer（还在建立连接）
	这时 resolver 又推送了新地址，需要更新哪个 balancer？
	应该更新 balancerPending（新的），而不是即将被淘汰的 balancerCurrent
	首次调用 都为nil 啥都没赋值
    */
    if gsb.balancerPending != nil {
        return gsb.balancerPending  // 是 balancerWrapper 结构体
    }
    return gsb.balancerCurrent
}

```

### `switchTo` 函数

```go
func (gsb *Balancer) switchTo(builder balancer.Builder) (*balancerWrapper, error) {
    // 1. 加锁
    gsb.mu.Lock()
    
    // 2. 检查是否已关闭（用 closed 字段，不是检查两个 balancer）
    if gsb.closed {
        gsb.mu.Unlock()
        return nil, errBalancerClosed
    }
    
    // 3. ★★★ 创建新的 balancerWrapper（但还没创建真正的 balancer！）★★★
    bw := &balancerWrapper{
        ClientConn: gsb.cc,           // ClientConn 包装器  本质上是ccBalancerWrapper
        builder:    builder,          // 你注册的 Builder
        gsb:        gsb,              // 指向父级
        lastState: balancer.State{
            // 初始状态：Connecting + 返回错误的 Picker
            ConnectivityState: connectivity.Connecting,
            Picker:            base.NewErrPicker(balancer.ErrNoSubConnAvailable),
        },
        subconns: make(map[balancer.SubConn]bool),  // SubConn 集合
    }
    
    // 4. 保存旧的 pending（可能是 nil）
    balToClose := gsb.balancerPending
    
    // 5. ★★★决定放到 current 还是 pending ★★★
    if gsb.balancerCurrent == nil {
        // 第一次调用：直接设为 current
        gsb.balancerCurrent = bw
    } else {
        // 已有 current：设为 pending（等待切换）
        gsb.balancerPending = bw
    }
    
    gsb.mu.Unlock()
    
    // 6. 关闭旧的 pending（如果有的话）
    // 注意：这里没有判断 nil，但 nil.Close() 会 panic！
    // 实际上 balancerWrapper.Close() 内部会判断 nil
    balToClose.Close()
    
    // 7. ★★★ 调用 builder.Build 创建真正的子 balancer ★★★
    // builder 是我们注册的 base.baseBuilder
    // bw 作为 ClientConn 传给子 balancer
    // 返回的是 base.baseBalancer
    newBalancer := builder.Build(bw, gsb.bOpts)
    
    // 8. 防御检查：Build 返回 nil 是非法的
    if newBalancer == nil {
        gsb.mu.Lock()
        if gsb.balancerPending != nil {
            gsb.balancerPending = nil
        } else {
            gsb.balancerCurrent = nil
        }
        gsb.mu.Unlock()
        return nil, balancer.ErrBadResolverState
    }

    // 9. 把真正的 balancer 存到 wrapper 里
    bw.Balancer = newBalancer
    
    return bw, nil
}
```

关于`balancerWrapper`的层级关系

```
CCBalancerWrapper 内部有个 balancer: 这个 balancer 调用的 switchTo 方法 生成一个真正的 balancerWrapper 负载均衡包装器 把这个挂载到 balancerCurrent 字段上 然后真正的 balancerWrapper 里面有我们自己定义最重要的pickerBuilder


CCBalancerWrapper{
	balancer {
		balancerCurrent： 真正balancerWrapper{
			我们自己放到globalselector 中的 builder 执行 build 后的  baseBalancer {
				pickerBuilder
			}
		}
	}
}

```



###  `builder.Build` 函数  

因为我用的`grpc` 的基础`Balancer`  所以调用也是基础的 `Build`

```GO
func (bb *baseBuilder) Build(cc balancer.ClientConn, _ balancer.BuildOptions) balancer.Balancer {
	bal := &baseBalancer{
        // cc 刚创建的 balancerWrapper
		cc:            cc,
		pickerBuilder: bb.pickerBuilder,    // 这个是重点 是我们自己写的 picker

      	// 地址 → SubConn 的映射
        // key: resolver.Address（包含 IP、端口、Attributes） 也就是我serviceInstance 变成grpc喜欢的
        // value: balancer.SubConn（底层 TCP 连接）  每一个 连接
        subConns: resolver.NewAddressMapV2[balancer.SubConn](),

        // SubConn → 状态 的映射
        // 记录每个 SubConn 当前是 Idle/Connecting/Ready/TransientFailure
        scStates: make(map[balancer.SubConn]connectivity.State),

        // 状态评估器
        // 根据所有 SubConn 的状态，计算整体状态
        // 比如：有一个 Ready 就整体 Ready，全部 Failure 就整体 Failure 
        csEvltr: &balancer.ConnectivityStateEvaluator{},

        //  配置（如是否开启健康检查）
        config: bb.config,

        // ★★★ 整体状态 ★★★
        // 这是 baseBalancer 自己的状态，初始是 Connecting
        state: connectivity.Connecting,
	}
	// 初始化 picker  在没有 Ready 的 SubConn 之前，返回错误 
	bal.picker = NewErrPicker(balancer.ErrNoSubConnAvailable)
	return bal
}

```

### `balToUpdate.UpdateClientConnState` 函数

```go
func (b *baseBalancer) UpdateClientConnState(s balancer.ClientConnState) error {
    // 清除之前的错误
    b.resolverErr = nil
    
    // 创建一个临时 Set，用于快速查找"哪些地址是新推送的"
    addrsSet := resolver.NewAddressMapV2[any]()
    // addrsSet 现在是空的 {}
    
    //  遍历新地址列表
    for _, a := range s.ResolverState.Addresses {
        // 把地址加入 Set
        addrsSet.Set(a, nil)
        // 举例 addrsSet = {
        //   "192.168.1.100:8080": nil,
        //   "192.168.1.101:8080": nil,
        //   "192.168.1.102:8080": nil,
        // }
        
        // 检查这个地址是否已经有 SubConn
        if _, ok := b.subConns.Get(a); !ok {
            // 第一次调用，b.subConns 是空的，所以每个地址都走这里
            
            // 创建 SubConn 选项
            var sc balancer.SubConn
            opts := balancer.NewSubConnOptions{
                HealthCheckEnabled: b.config.HealthCheck,  // 是否健康检查
                // ★★★ 状态监听器 ★★★
                // 当 SubConn 状态变化时，gRPC 会调用这个函数
                StateListener: func(scs balancer.SubConnState) { 
                    b.updateSubConnState(sc, scs) 
                },
            }
            
            //  ★★★ 创建 SubConn（TCP 连接）★★★
            sc, err := b.cc.NewSubConn([]resolver.Address{a}, opts)
            if err != nil {
                continue
            }
            
            // 保存到 subConns map
            b.subConns.Set(a, sc)
            // b.subConns = {
            //   "192.168.1.100:8080": SubConn1,
            //   "192.168.1.101:8080": SubConn2,
            //   "192.168.1.102:8080": SubConn3,
            // }
            
            // 初始化状态为 Idle
            b.scStates[sc] = connectivity.Idle
            // b.scStates = {
            //   SubConn1: Idle,
            //   SubConn2: Idle,
            //   SubConn3: Idle,
            // }
            
            // 记录状态转换（从 Shutdown 到 Idle）
            // csEvltr 用来计算整体状态
            b.csEvltr.RecordTransition(connectivity.Shutdown, connectivity.Idle)
            
            // ★★★ 开始连接 ★★★
            sc.Connect()
            // 此时 SubConn1/2/3 开始 TCP 握手
        }
    }
    
    // 清理已下线的地址（第一次调用时 subConns 和 addrsSet 一样，不会清理）
    // 如果第n次执行 本来有三个 subConns 现在 address 有两个 说明我要关闭一个 subConns
    for _, a := range b.subConns.Keys() {
        sc, _ := b.subConns.Get(a)
        if _, ok := addrsSet.Get(a); !ok {
            // 地址不在新列表里了 → 服务下线了
            sc.Shutdown()
            b.subConns.Delete(a)
        }
    }
    
    // 如果没有地址，返回错误
    if len(s.ResolverState.Addresses) == 0 {
        b.ResolverError(errors.New("produced zero addresses"))
        return balancer.ErrBadResolverState
    }

    // 重新生成 Picker（但此时还没有 Ready 的 SubConn）  
    // 之前 bal.picker = NewErrPicker(balancer.ErrNoSubConnAvailable)
    b.regeneratePicker()
    // b.picker = ErrPicker（因为没有 Ready 的）
    
    // 通知上层状态  调用的是 balancerWrapper.UpdateState
    b.cc.UpdateState(balancer.State{
        ConnectivityState: b.state,  // Connecting
        Picker: b.picker,            // ErrPicker
    })
    
    return nil
}
```

### `baseBalancer.regeneratePicker` 函数

```go
func (b *baseBalancer) regeneratePicker() {
    // 1. 如果整体状态是失败，返回错误 Picker
    if b.state == connectivity.TransientFailure {
        b.picker = NewErrPicker(b.mergeErrors())
        return
    }
    /*
    b.subConns = {
    Address{Addr:"192.168.1.100:8080", Attributes:{rawServiceInstance: ins1}} → SubConn1,
    Address{Addr:"192.168.1.101:8080", Attributes:{rawServiceInstance: ins2}} → SubConn2,
    Address{Addr:"192.168.1.102:8080", Attributes:{rawServiceInstance: ins3}} → SubConn3,
}

	b.scStates = {
   	 	SubConn1 → Ready,
    	SubConn2 → Ready,
    	SubConn3 → Connecting,  // 还在连接中 这个就不要
}

readySCs = {
    SubConn1 → SubConnInfo{Address: Address{Addr:"192.168.1.100:8080", Attributes:...}},
    SubConn2 → SubConnInfo{Address: Address{Addr:"192.168.1.101:8080", Attributes:...}},
}
    */
    // 收集所有 Ready 的 SubConn
    readySCs := make(map[balancer.SubConn]SubConnInfo)
    for _, addr := range b.subConns.Keys() {
        sc, _ := b.subConns.Get(addr)
        if st, ok := b.scStates[sc]; ok && st == connectivity.Ready {
            readySCs[sc] = SubConnInfo{Address: addr}
        }
    }
    
    // 调用你的 Build 创建 Picker
    b.picker = b.pickerBuilder.Build(PickerBuildInfo{ReadySCs: readySCs})
}
```

### 自己的 `PickerBuilder.Build` 函数

```GO
func (b *balancerBuilder) Build(info base.PickerBuildInfo) balancer.Picker {
    // 没有 Ready 的 SubConn，返回错误 Picker
    if len(info.ReadySCs) == 0 {
        return base.NewErrPicker(balancer.ErrNoSubConnAvailable)
    }
    
    // 把 gRPC 的 SubConn 转成自己的 Node
    nodes := make([]selector.Node, 0, len(info.ReadySCs))
    for conn, info := range info.ReadySCs {
        // 从 Attributes 取出我们 resolver 里存的 ServiceInstance   这样就不用转换了
        ins, _ := info.Address.Attributes.Value("rawServiceInstance").(*registry.ServiceInstance)
        
        //  创建 grpcNode（包装 SubConn 和 Node）
        nodes = append(nodes, &grpcNode{
            Node:    selector.NewNode("grpc", info.Address.Addr, ins),
            subConn: conn,  // ★ 保存 SubConn，后面 Pick 要用
        })
    }
    
    // 用 selector.Builder 创建 Selector
    p := &balancerPicker{
        selector: b.builder.Build(),  // GlobalSelector().Build()
    }
    
    // 把节点列表应用到 Selector 
    p.selector.Apply(nodes)
    
    return p
}
```

### `b.builder.Build()` 函数

比如用 随机

```go
// 无论是随机 还是 p2c 还是 wrr 都走这个 因为 我们传递给globalselector 都是 用的一个 DefaultBuilder
func (db *DefaultBuilder) Build() Selector {
	return &Default{
		NodeBuilder: db.Node,
		Balancer:    db.Balancer.Build(),  // 这个就是具体的 什么策略
	}
}

//  db.Balancer.Build()
func (b *Builder) Build() selector.Balancer {
	return &Balancer{}  //都是自己定义的结构体  比如 p2c也是返回他自己定义的Balancer  只要实现pick方法就好
}
```

### `Apply` 方法

```GO
func (d *Default) Apply(nodes []Node) {
	weightedNodes := make([]WeightedNode, 0, len(nodes))
	for _, n := range nodes {
		weightedNodes = append(weightedNodes, d.NodeBuilder.Build(n))
	}
	// TODO: Do not delete unchanged nodes
	d.nodes.Store(weightedNodes)
}

```

### 真正的 `balancerWrapper` 的 `UpdateState` 方法 （向上反馈）

```GO
func (bw *balancerWrapper) UpdateState(state balancer.State) {
	// Hold the mutex for this entire call to ensure it cannot occur
	// concurrently with other updateState() calls. This causes updates to
	// lastState and calls to cc.UpdateState to happen atomically.
	bw.gsb.mu.Lock()
	defer bw.gsb.mu.Unlock()
	bw.lastState = state

	if !bw.gsb.balancerCurrentOrPending(bw) {
		return
	}

	if bw == bw.gsb.balancerCurrent {
		// In the case that the current balancer exits READY, and there is a pending
		// balancer, you can forward the pending balancer's cached State up to
		// ClientConn and swap the pending into the current. This is because there
		// is no reason to gracefully switch from and keep using the old policy as
		// the ClientConn is not connected to any backends.
		if state.ConnectivityState != connectivity.Ready && bw.gsb.balancerPending != nil {
			bw.gsb.swap()
			return
		}
		// Even if there is a pending balancer waiting to be gracefully switched to,
		// continue to forward current balancer updates to the Client Conn. Ignoring
		// state + picker from the current would cause undefined behavior/cause the
		// system to behave incorrectly from the current LB policies perspective.
		// Also, the current LB is still being used by grpc to choose SubConns per
		// RPC, and thus should use the most updated form of the current balancer.
		bw.gsb.cc.UpdateState(state)
		return
	}
	// This method is now dealing with a state update from the pending balancer.
	// If the current balancer is currently in a state other than READY, the new
	// policy can be swapped into place immediately. This is because there is no
	// reason to gracefully switch from and keep using the old policy as the
	// ClientConn is not connected to any backends.
	if state.ConnectivityState != connectivity.Connecting || bw.gsb.balancerCurrent.lastState.ConnectivityState != connectivity.Ready {
		bw.gsb.swap()
	}
}
```

### `bw.gsb.cc.UpdateState` 函数

```GO
func (ccb *ccBalancerWrapper) UpdateState(s balancer.State) {
	ccb.cc.mu.Lock()
	defer ccb.cc.mu.Unlock()
	if ccb.cc.conns == nil {
		// The CC has been closed; ignore this update.
		return
	}

	ccb.mu.Lock()
	if ccb.closed {
		ccb.mu.Unlock()
		return
	}
	ccb.mu.Unlock()
	// Update picker before updating state.  Even though the ordering here does
	// not matter, it can lead to multiple calls of Pick in the common start-up
	// case where we wait for ready and then perform an RPC.  If the picker is
	// updated later, we could call the "connecting" picker when the state is
	// updated, and then call the "ready" picker after the picker gets updated.

	// Note that there is no need to check if the balancer wrapper was closed,
	// as we know the graceful switch LB policy will not call cc if it has been
	// closed.
	ccb.cc.pickerWrapper.updatePicker(s.Picker)
	ccb.cc.csMgr.updateState(s.ConnectivityState)
}
```





### `ScheduleOr` 函数

```GO
func (cs *CallbackSerializer) ScheduleOr(f func(ctx context.Context), onFailure func()) {
	if cs.callbacks.Put(f) != nil {
		onFailure()
	}
}

```



整体流程

```
我们自己 用  selector.SetGlobalSelector(p2c.NewBuilder()) 或者selector.SetGlobalSelector(random.NewBuilder())
填上 全局 slector  然后执行 InitBuilder 注册到 grpc
用户拨号 进入 DialContext 函数：
1： NewClient 作用初始化一系列东西 重点：
A: defaultServiceConfigRawJSON 
解析我们送进来的 {"loadBalancingPolicy":"selector"} 还有个性化配置 本质上就是找到我们注册进去的 BalanceBuilder
B: 还有初始化 CCBalancerWrapper 这个上面有详细流程图  后续用于 ccb.balancer.UpdateClientConnState 函数
C: 创建连接状态管理器等一系列管理器

2： ExitIdleMode 函数 
调用  cc.resolverWrapper.start()
会调用我们自己的服务发现的build方法 ccr.resolver, err = ccr.cc.resolverBuilder.Build(ccr.cc.parsedTarget, ccr, opts)
返回实现Resolver接口的结构体  我们调用真正服务发现的watch方法 因为 这层类似接口层 不会写死用什么 当时注册什么 就用它的watch 要注册必须实现这个方法 拿consul举例子 返回 watcher 代表一个 订阅方，详细去看服务发现，执行本地update 作用把 自己拿到的服务实例 地址端口啥的 变成grpc能接收的结构体 然后调用 它的 UpdateState 在这个方法里调用updateResolverStateAndUnlock这个方法 首先第一个点 它会解析你是不是动态负载均衡  无论是不是 本质上都是拿到 我们自己注册的 负载均衡的 InitBuilder  拿到后 进入 ccbalancer的updateClientConnState 方法 这个时候 函数的形参从原来的 只有服务实例列表 变成 服务实例列表 + lbConfig里面是我们的负载均衡builder
这个方法 有2件事情：
第一： 执行ccbalancer.balancer.UpdateClientConnState 这个方法
第二：执行调度  ccbalancer.serializer.Schedule （后面细说）
首先第一个函数：
首次调用或者动态策略变更 要执行 switchTo 函数 用于创建策略或者更新策略，执行 switchTo 方法 创建真正的 balancerWrapper 并且把我们传入的负载均衡builder 在这进行build方法 由于我们使用的 base方法 就直接返回的是 basebalancer 结构体挂到balancerWrapper 上  参数很重要 有地址 → SubConn 的映射 还有 记录SubConn什么状态的字段等等，这个是首次调用或者变更 需要的初始化 这个时候已经拿到了真正的balancerWrapper和basebalancer 需要执行basebalancer.UpdateClientConnState(state)
参数依然是那些，这个其实是我们自己的代码 但是由于我们用的是grpc的base 所以还是调用他们的 如果你自己写一个initBuilder 返回的是自己定义的符合接口的 那这个地方就会调用你自己写的这个函数 ，这个函数主要是 填充刚才新建的字段 地址 → SubConn 的映射还有空闲字段，回答一个疑问 为啥不在初始化的时候就填上，如果第二次来，而不是第一次，那不会走switchTo 这个函数 正好有个实例下线了 咋更新呢，所以一定要分开，switchTo 只负责新建 UpdateClientConnState 负责填入或者更新 ， UpdateClientConnState函数取出我们当时存的ServiceInstance 然后包装成自己的Node方便使用，最重要的这个函数会执行pickerBuilder的build方法也就是globalselector的build方法 这个之前就说过是一个包装层 不被写死所以所有的负载均衡策略都走这个，在这个包装层build方法种会执行真正pickerBuilder的build方法创建真正的picker，也就是包装层字段就有我们传入的比如随机策略、p2c策略等等picker 都是实现了 pick方法的  

```

